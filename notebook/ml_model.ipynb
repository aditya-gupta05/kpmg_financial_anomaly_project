{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67342218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../data/creditcard_cleaned.csv\")\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "298debfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE class counts (train):\n",
      "Class\n",
      "0    226602\n",
      "1       378\n",
      "Name: count, dtype: int64\n",
      "After SMOTE class counts (train_res):\n",
      "Class\n",
      "0    226602\n",
      "1    226602\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "print(\"Before SMOTE class counts (train):\")\n",
    "print(y_train.value_counts())\n",
    "print(\"After SMOTE class counts (train_res):\")\n",
    "print(pd.Series(y_train_sm).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14cb2d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9996    0.9999    0.9997     56651\n",
      "           1     0.9231    0.7579    0.8324        95\n",
      "\n",
      "    accuracy                         0.9995     56746\n",
      "   macro avg     0.9613    0.8789    0.9161     56746\n",
      "weighted avg     0.9995    0.9995    0.9995     56746\n",
      "\n",
      "Confusion Matrix:\n",
      " [[56645     6]\n",
      " [   23    72]]\n",
      "Accuracy: 0.9994889507630493\n",
      "Average Precision Score: 0.8113838895960894\n"
     ]
    }
   ],
   "source": [
    "# Random Forest - SMOTE, No Class Weights\n",
    "rf_sm = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_sm.fit(X_train_sm, y_train_sm)\n",
    "y_pred = rf_sm.predict(X_test)\n",
    "y_proba = rf_sm.predict_proba(X_test)[:, 1]\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Average Precision Score:\", average_precision_score(y_test, y_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b05d275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3fc0a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 global feature importances:\n",
      "V14    0.227986\n",
      "V10    0.113291\n",
      "V17    0.104117\n",
      "V4     0.088475\n",
      "V12    0.078795\n",
      "V11    0.071133\n",
      "V3     0.060606\n",
      "V16    0.046264\n",
      "V2     0.025569\n",
      "V7     0.024112\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "feature_importances = pd.Series(rf_sm.feature_importances_, index=X.columns)\n",
    "feature_importances = feature_importances.sort_values(ascending=False)\n",
    "print(\"\\nTop 10 global feature importances:\")\n",
    "print(feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c1ba25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top flagged (predicted fraud) audit notes (top 10 by probability):\n",
      "  idx  Probability                                                                                                               Audit_Note\n",
      "24442          1.0    Prediction: FRAUD (prob=1.000). Top features (global): V14=-9.2661, V10=-5.6536, V17=-5.7091, V4=5.0370, V12=-5.8837.\n",
      "30966          1.0 Prediction: FRAUD (prob=1.000). Top features (global): V14=-6.5526, V10=-15.1238, V17=-20.1645, V4=6.2074, V12=-14.1750.\n",
      "30110          1.0   Prediction: FRAUD (prob=1.000). Top features (global): V14=-12.0438, V10=-5.1728, V17=-1.8799, V4=7.3165, V12=-5.6404.\n",
      "52997          1.0 Prediction: FRAUD (prob=1.000). Top features (global): V14=-9.8872, V10=-12.6959, V17=-21.7102, V4=8.6986, V12=-11.9609.\n",
      "20871          1.0   Prediction: FRAUD (prob=1.000). Top features (global): V14=-7.8409, V10=-4.1389, V17=-9.9318, V4=5.9282, V12=-11.1240.\n",
      "36033          1.0    Prediction: FRAUD (prob=1.000). Top features (global): V14=-6.5275, V10=-5.9274, V17=-6.7006, V4=4.3333, V12=-5.7885.\n",
      "36050          1.0    Prediction: FRAUD (prob=1.000). Top features (global): V14=-6.7782, V10=-5.9966, V17=-6.8576, V4=4.3034, V12=-5.9623.\n",
      "27985          1.0    Prediction: FRAUD (prob=1.000). Top features (global): V14=-7.2799, V10=-6.1349, V17=-7.1717, V4=4.2431, V12=-6.3098.\n",
      "25285          1.0    Prediction: FRAUD (prob=1.000). Top features (global): V14=-7.6787, V10=-5.5263, V17=-5.8443, V4=4.5225, V12=-8.5255.\n",
      "48877          1.0    Prediction: FRAUD (prob=1.000). Top features (global): V14=-4.4521, V10=-2.7558, V17=-5.2486, V4=2.0641, V12=-3.5215.\n"
     ]
    }
   ],
   "source": [
    "# Prepare for audit notes\n",
    "X_test_reset = X_test.reset_index(drop=True)\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "y_pred_series = pd.Series(y_pred, index=X_test_reset.index)\n",
    "y_proba_series = pd.Series(y_proba, index=X_test_reset.index)\n",
    "\n",
    "# Helper: create a short \"top features for this row\" using the global top features\n",
    "global_top_feats = list(feature_importances.head(5).index)\n",
    "\n",
    "def top_feature_values_for_row(row):\n",
    "    pairs = []\n",
    "    for feat in global_top_feats:\n",
    "        val = row[feat]\n",
    "        pairs.append(f\"{feat}={val:.4f}\")\n",
    "    return \", \".join(pairs)\n",
    "\n",
    "audit_rows = []\n",
    "for i in X_test_reset.index:\n",
    "    row = X_test_reset.loc[i]\n",
    "    pred = int(y_pred_series.loc[i])\n",
    "    prob = float(y_proba_series.loc[i])\n",
    "    actual = int(y_test_reset.loc[i])\n",
    "    top_vals = top_feature_values_for_row(row)\n",
    "    status = (\n",
    "        \"True Positive\" if (pred == 1 and actual == 1) else\n",
    "        \"False Positive\" if (pred == 1 and actual == 0) else\n",
    "        \"True Negative\" if (pred == 0 and actual == 0) else\n",
    "        \"False Negative\"\n",
    "    )\n",
    "    note = (\n",
    "        f\"Prediction: {'FRAUD' if pred==1 else 'NOT FRAUD'} \"\n",
    "        f\"(prob={prob:.3f}). \"#Actual: {'FRAUD' if actual==1 else 'NOT FRAUD'} â€” {status}. \"\n",
    "        f\"Top features (global): {top_vals}.\"\n",
    "    )\n",
    "    audit_rows.append({\n",
    "        \"idx\": i,\n",
    "        \"Prediction\": pred,\n",
    "        \"Probability\": prob,\n",
    "        \"Actual\": actual,\n",
    "        \"Status\": status,\n",
    "        \"Audit_Note\": note\n",
    "    })\n",
    "\n",
    "audit_df = pd.DataFrame(audit_rows).sort_values(by=\"Probability\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save or inspect the top flagged rows\n",
    "print(\"\\nTop flagged (predicted fraud) audit notes (top 10 by probability):\")\n",
    "print(audit_df[audit_df[\"Prediction\"]==1].head(10)[[\"idx\",\"Probability\",\"Audit_Note\"]].to_string(index=False))\n",
    "\n",
    "# Optionally: save audit notes to CSV\n",
    "audit_df.to_csv(\"../data/audit_notes_from_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ffcda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
